\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage[document]{ragged2e}

\usepackage{amsfonts}
\usepackage{natbib}
\usepackage[dvipsnames]{xcolor}
\usepackage{graphicx}

\usepackage[fleqn]{amsmath}
\usepackage{amssymb}

\usepackage{minted}

\title{Algoritmo de Descenso Gradiente}
\author{Victor Gerardo Rodríguez Barragán}
\date{16 de Septiembre del 2023}

\begin{document}
\maketitle
\begin{center}
    \includegraphics[width=0.8\textwidth]{/home/gerry/Documents/ceti/cetilogo.jpg}
\end{center}

\newpage
\justify
\section*{Ejercicio}
El ejercicio consta de implementar el algoritmo de descenso gradiente para encontrar el minimo
de una funcion, en este caso la funcion es $f(x) = x^2 + 5sin(x)$.

El algoritmo se puede utilizar en el aprendizaje de maquina, en un modelo de regresion lineal
para encontrar los coeficientes de la funcion de regresion, entre otras aplicaciones.

\section*{Codigo}
\begin{minted}{python}
import numpy as np

def sr1(objfun, gradient, init, tolerance=1e-6, maxiter=10000):
    x = np.array(init)
    iterno = 0
    B = np.identity(2)
    xarray = [x]
    fprev = objfun(x[0],x[1])
    farray = [fprev]
    gprev = gradient(x[0],x[1])
    xtmp = x - 0.01*gprev/np.sqrt(np.dot(gprev,gprev))
    gcur = gradient(xtmp[0],xtmp[1])
    s = xtmp-x
    y = gcur-gprev
    while iterno < maxiter:
        r = y-np.dot(B,s)
        B = B + np.outer(r,r)/np.dot(r,s)        
        x = x - np.linalg.solve(B,gcur)
        fcur = objfun(x[0], x[1])
        if np.isnan(fcur):
            break
        gprev = gcur
        gcur = gradient(x[0],x[1])
        xarray.append(x)
        farray.append(fcur)
        if abs(fcur-fprev)<tolerance:
            break
        fprev = fcur
        s = xarray[-1]-xarray[-2]
        y = gcur-gprev
        iterno += 1
    return np.array(xarray), np.array(farray)

# Creación de un conjunto de datos para entrenamiento
trX = np.linspace(-2, 2, 101)
trY = 3 + 2 * trX + np.random.randn(*trX.shape) * 0.33

# Definición de los ajustes y parámetros iniciales
num_steps = 100
learningRate = 0.10
criteria = 1e-8
b_0 = 1
b_1 = 1

# Proceso iterativo
for step in range(0, num_steps):
    b_0_gradient = 0
    b_1_gradient = 0
    N = float(len(trX))

    for i in range(0, len(trX)):
        b_0_gradient -= (2/N) * (trY[i] - (b_0 + b_1 * trX[i]))
        b_1_gradient -= (2/N) * (trY[i] - (b_0 + b_1 * trX[i])) * trX[i]
        
    b_0 = b_0 - (learningRate * b_0_gradient)
    b_1 = b_1 - (learningRate * b_1_gradient)

    if max(abs(learningRate * b_0_gradient), abs(learningRate * b_1_gradient)) < criteria:
        break
    
# Impresión de los resultados
print("Los valores que se obtienen son:", b_0, b_1, "en pasos", step)

# + El código de la función de gradiente proporcionada por el profesor.
\end{minted}

\begin{center}
    \includegraphics[width=0.8\textwidth]{/home/gerry/Pictures/screenshots/select/screenshot2023-09-16_19:51-06.png}
    \includegraphics[width=0.8\textwidth]{/home/gerry/Pictures/screenshots/full/screenshot2023-09-16_19:46-25.png}
    \includegraphics[width=0.8\textwidth]{/home/gerry/Pictures/screenshots/full/download.png}
\end{center}

\section*{Conclusion}
La teoria del algoritmo de Dijkstra es ``en teoria'' sencilla, pero en la practica se me hizo
mucho mas compleja, tuve que aplicar estructuras de datos que me
complicaron un poco el codigo, pero al final logre implementar la practica. Una
de las cosas que mas me costo trabajo fue implementar el algoritmo como tal en Rust,
apesar de haber usado el lenguaje con anterioridad nunca habia implementado un algoritmo como
este, al final use muchas cosas como heap binarios, tablas de hash, etc. que me ayudaron a
la implementacion del algoritmo, porque con puros Vectores deduje que seria muy complicado, me gustaria
mejorar el codigo a lo mejor a lo mejor usando dict o algun otro tipo que facilite y haga mas
legible el codigo.


\end{document}
