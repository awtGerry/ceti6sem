\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage[document]{ragged2e}

\usepackage{amsfonts}
\usepackage{natbib}
\usepackage[dvipsnames]{xcolor}
\usepackage{graphicx}

\usepackage[fleqn]{amsmath}
\usepackage{amssymb}

\usepackage{minted}

\title{Algoritmo de Descenso Gradiente}
\author{Victor Gerardo Rodríguez Barragán}
\date{16 de Septiembre del 2023}

\begin{document}
\maketitle
\begin{center}
    \includegraphics[width=0.8\textwidth]{/home/gerry/Documents/ceti/cetilogo.jpg}
\end{center}

\newpage
\justify
\section*{Ejercicio}
El ejercicio consta de implementar el algoritmo de descenso gradiente para encontrar el minimo
de una funcion, en este caso la funcion es $f(x) = x^2 + 5sin(x)$.

El algoritmo se puede utilizar en el aprendizaje de maquina, en un modelo de regresion lineal
para encontrar los coeficientes de la funcion de regresion, entre otras aplicaciones.

\section*{Codigo}
\begin{minted}{python}
import numpy as np

def sr1(objfun, gradient, init, tolerance=1e-6, maxiter=10000):
    x = np.array(init)
    iterno = 0
    B = np.identity(2)
    xarray = [x]
    fprev = objfun(x[0],x[1])
    farray = [fprev]
    gprev = gradient(x[0],x[1])
    xtmp = x - 0.01*gprev/np.sqrt(np.dot(gprev,gprev))
    gcur = gradient(xtmp[0],xtmp[1])
    s = xtmp-x
    y = gcur-gprev
    while iterno < maxiter:
        r = y-np.dot(B,s)
        B = B + np.outer(r,r)/np.dot(r,s)        
        x = x - np.linalg.solve(B,gcur)
        fcur = objfun(x[0], x[1])
        if np.isnan(fcur):
            break
        gprev = gcur
        gcur = gradient(x[0],x[1])
        xarray.append(x)
        farray.append(fcur)
        if abs(fcur-fprev)<tolerance:
            break
        fprev = fcur
        s = xarray[-1]-xarray[-2]
        y = gcur-gprev
        iterno += 1
    return np.array(xarray), np.array(farray)

# Creación de un conjunto de datos para entrenamiento
trX = np.linspace(-2, 2, 101)
trY = 3 + 2 * trX + np.random.randn(*trX.shape) * 0.33

# Definición de los ajustes y parámetros iniciales
num_steps = 100
learningRate = 0.10
criteria = 1e-8
b_0 = 1
b_1 = 1

# Proceso iterativo
for step in range(0, num_steps):
    b_0_gradient = 0
    b_1_gradient = 0
    N = float(len(trX))

    for i in range(0, len(trX)):
        b_0_gradient -= (2/N) * (trY[i] - (b_0 + b_1 * trX[i]))
        b_1_gradient -= (2/N) * (trY[i] - (b_0 + b_1 * trX[i])) * trX[i]
        
    b_0 = b_0 - (learningRate * b_0_gradient)
    b_1 = b_1 - (learningRate * b_1_gradient)

    if max(abs(learningRate * b_0_gradient), abs(learningRate * b_1_gradient)) < criteria:
        break
    
# Impresión de los resultados
print("Los valores que se obtienen son:", b_0, b_1, "en pasos", step)

# + El código de la función de gradiente proporcionada por el profesor.
\end{minted}

\begin{center}
    \includegraphics[width=0.8\textwidth]{/home/gerry/Pictures/screenshots/select/screenshot2023-09-16_19:51-06.png}
    \includegraphics[width=0.8\textwidth]{/home/gerry/Pictures/screenshots/full/screenshot2023-09-16_19:46-25.png}
    \includegraphics[width=0.8\textwidth]{/home/gerry/Pictures/screenshots/full/download.png}
\end{center}

\section*{Conclusion}
Es un algoritmo muy interesante. Me gustaria aprender mas sobre el, y sobre el aprendizaje de
maquina en general. Me gustaria aprender a utilizarlo mas adelante pues esta vez no pude
implementarlo en el ejercicio como me hubiera gustado. Python es un lenguaje que personalmente
nuncas habia utilizado, no tuve el tiempo suficiente para aprenderlo y me gustaria intentar implementar
este algoritmo en C o Rust aplicandolo a un problema de regresion lineal con OpenGL o algo por el estilo.

\end{document}
